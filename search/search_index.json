{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"HomeAssistant Remote Logger","text":""},{"location":"#home-assistant-remote-logger","title":"Home Assistant Remote Logger","text":"<p>Listens to Home Assistant system log events and sends structured log events to a remote Syslog or OpenTelemetry (OTLP) collector.</p> <p></p> <p>Logs structure is preserved from the Home Assistant internal event, so multi-line logs and stacktraces are preserved as single log entries, unlike console scrapers which will create a log event per line, and will capture script names, line numbers and versions properly.</p> <p>Only Home Assistant server itself, with its custom components is supported. Logs from apps (previously known as 'add-ins'), HAOS or the HA Supervisor aren't provided as events to be captured, so require an alternative solution, like Bert Baron's LogSpout Home Assistant App will cover these. It can be used in combination with Remote Logger so that Home Assistant has good structured logs, and everything else is at least logged.</p>"},{"location":"#installation","title":"Installation","text":"<p>Remote Logger is a HACS component, so that has to be installed first, using the instructions at Getting Started with HACS.</p> <p>The integration installs using the Home Assistant integrations page, and has no YAML configuration.</p> <p></p> <p>However, a YAML change is required to the Home Assistant System Log integration, to enable event forwarding.</p> <p><code>yaml title=\"Home Assistant Configuration\" system_log:     fire_event: true</code></p>"},{"location":"#open-telemetry-otel","title":"Open Telemetry (OTEL)","text":"<p>Logs are sent using Open Telemetry Logs specification over a Open Telemetry Protocol(OTLP) connection, either as Protobuf or JSON, and currently only as HTTP (gRPC may be added in future).</p> <p></p> <p>For more information, see OpenTelemetry Logging.</p> <p>Log records are collected and sent in a bundle</p>"},{"location":"#syslog","title":"Syslog","text":"<p>Messages are sent using the more recent RFC5424 format, with additional structured data using OTEL taxonomy (see Additional Attributes).</p> <p>Syslog can be sent as TCP or UDP.</p> <p></p>"},{"location":"#additional-attributes","title":"Additional Attributes","text":"<p>The following additional attributes, derived directly from the Home Assistant log event, are provided as Syslog <code>STRUCTURED-DATA</code> attributes, or OTEL attributes.</p> <ul> <li><code>code.file.path</code></li> <li><code>code.line.number</code></li> <li><code>code.function.name</code> (this is the <code>logger</code> value from Home Assistant)</li> <li><code>exception.count</code></li> <li><code>exception.first_occurred</code></li> <li><code>exception.stacktrace</code></li> </ul> <p>OTEL taxonomy is used for both OTEL and Syslog since there's no standard taxonomy at this level of Syslog.</p>"},{"location":"#log-servers","title":"Log Servers","text":"<p>There are a zillion possible solutions for capturing, analyzing, aggregating and storing logs.</p> <p>One combination that works well is using Vector and GreptimeDb - they are fast, lightweight, open source, customizable and run under Docker. Vector has support for OTEL logging, as well as Syslog, and has good remapping ability to fine tune each source. Its then easy to pull in logs from Docker servers, firewalls, Unifi switches or wherever else into one time-line, as well as server and network metrics.</p>"},{"location":"#diagnostic-entities","title":"Diagnostic Entities","text":"<p>Home Assistant sensors are created and updated to monitor log activity, plus any errors either generating log messages or posting them to remote servers.</p> <p></p>"},{"location":"changelog/","title":"1.1.0","text":"<ul> <li>More customization for OTLP, now able to use more platforms, including OpenObserve<ul> <li>Custom HTTP headers</li> <li>Bearer token</li> <li>Alternative API endpoint or custom path prefix</li> </ul> </li> </ul>"},{"location":"changelog/#100","title":"1.0.0","text":"<ul> <li>Dependencies and docs update for public release</li> </ul>"},{"location":"changelog/#040","title":"0.4.0","text":"<ul> <li>Add Device to bundle entities</li> </ul>"},{"location":"changelog/#030","title":"0.3.0","text":"<ul> <li>Unique names for exporters</li> </ul>"},{"location":"changelog/#020","title":"0.2.0","text":"<ul> <li>Add entities to track incoming events, outgoing postings, errors</li> <li>Refactor to common exporter class</li> </ul>"},{"location":"changelog/#005","title":"0.0.5","text":"<ul> <li>Use the OpenTelemetry convention of prefixing Syslog structured data parameters with the non-IANA standard <code>opentelemetry</code></li> <li>Messages retried if connection down</li> </ul>"},{"location":"examples/vector_greptimedb/","title":"Index","text":"<p>These are working examples, not ideal recommendations.</p> <p>The Vector config will also pull in Docker logs and systemd journal from the box on which it runs ( excluding Vector and Greptime from the Docker logging to prevent cycles).</p> <p>{{ pagetree(siblings) }}</p>"},{"location":"examples/vector_greptimedb/docker-compose/","title":"Example Docker Compose","text":"<pre><code>services:\n  vector:\n    image: timberio/vector:latest-debian\n    container_name: vector\n    user: \"994:994\"\n    group_add:\n      - \"119\" # docker group, for access to daemon socket\n      - \"101\" # systemd-journal group\n    command: --config /etc/vector/vector.yaml\n    volumes:\n      - ./conf:/etc/vector\n      - ./logs:/logs\n      - /var/run/docker.sock:/var/run/docker.sock\n      - ./vector_data:/var/vector\n    ports:\n      - 8686:8686\n      - 514:514/udp\n      - 5514:5514/tcp\n      - 4317:4317/tcp # otel gRPC\n      - 4318:4318/tcp # otel http\n    #environment:\n        #- VECTOR_LOG=debug,hyper=debug,h2=debug\n    cap_add:\n      - CAP_NET_BIND_SERVICE\n    restart: always\n  greptime:\n    image: greptime/greptimedb:latest\n    container_name: greptime\n    restart: always\n    volumes:\n      - ./greptimedb_data:/greptimedb_data\n      - ./conf/greptime.toml:/etc/greptime/config.toml\n    ports:\n      - 4000:4000\n      - 4001:4001\n      - 4002:4002\n      - 4003:4003\n    command:\n      - standalone\n      - start\n      - --http-addr=0.0.0.0:4000\n      - --rpc-bind-addr=0.0.0.0:4001\n      - --mysql-addr=127.0.0.1:4002\n      - --postgres-addr=127.0.0.1:4003\n      - --config-file=/etc/greptime/config.toml\n\n</code></pre>"},{"location":"examples/vector_greptimedb/greptime.toml/","title":"Example GreptimeDB confoig","text":"<pre><code>## The default timezone of the server.\ndefault_timezone = \"Europe/London\"\n\n## The default column prefix for auto-created time index and value columns.\ndefault_column_prefix = \"greptime\"\n\n## Initialize all regions in the background during the startup.\n## By default, it provides services after all regions have been initialized.\ninit_regions_in_background = false\n\n## Parallelism of initializing regions.\ninit_regions_parallelism = 16\n\n## The maximum current queries allowed to be executed. Zero means unlimited.\n## NOTE: This setting affects scan_memory_limit's privileged tier allocation.\n## When set, 70% of queries get privileged memory access (full scan_memory_limit).\n## The remaining 30% get standard tier access (70% of scan_memory_limit).\nmax_concurrent_queries = 3\n\n## Enable telemetry to collect anonymous usage data. Enabled by default.\nenable_telemetry = false\n\n[http]\naddr = \"0.0.0.0:4000\"\n## HTTP request timeout. Set to 0 to disable timeout.\ntimeout = \"0s\"\n## HTTP request body limit.\n## The following units are supported: `B`, `KB`, `KiB`, `MB`, `MiB`, `GB`, `GiB`, `TB`, `TiB`, `PB`, `PiB`.\n## Set to 0 to disable limit.\nbody_limit = \"64MB\"\nenable_cors = true\nprom_validation_mode = \"lossy\"\n\n[mysql]\nenable = true\naddr = \"127.0.0.1:4002\"\n## The number of server worker threads.\nruntime_size = 2\n## Server-side keep-alive time.\n## Set to 0 (default) to disable.\nkeep_alive = \"0s\"\n## Maximum entries in the MySQL prepared statement cache; default is 10,000.\nprepared_stmt_cache_size= 10000\n# MySQL server TLS options.\n\n[postgres]\nenable = true\naddr = \"127.0.0.1:4003\"\n## The number of server worker threads.\nruntime_size = 2\n## Server-side keep-alive time.\n## Set to 0 (default) to disable.\nkeep_alive = \"0s\"\n\n[opentsdb]\nenable = true\n\n[influxdb]\nenable = true\n\n[jaeger]\nenable = false\n\n[prom_store]\nenable = true\n## Whether to store the data from Prometheus remote write in metric engine.\nwith_metric_engine = true\n\n[wal]\nprovider = \"raft_engine\"\ndir = \"./greptimedb_data/wal\"\nfile_size = \"128MB\"\npurge_threshold = \"1GB\"\npurge_interval = \"1m\"\nread_batch_size = 128\nsync_write = false\nenable_log_recycle = true\nprefill_log_files = false\nsync_period = \"10s\"\nrecovery_parallelism = 2\noverwrite_entry_start_id = false\n\n[metadata_store]\nfile_size = \"64MB\"\npurge_threshold = \"256MB\"\npurge_interval = \"1m\"\n\n[procedure]\nmax_retry_times = 3\nretry_delay = \"500ms\"\nmax_running_procedures = 128\n\n[flow]\n[query]\nparallelism = 0\n\n## Memory pool size for query execution operators (aggregation, sorting, join).\n## Supports absolute size (e.g., \"2GB\", \"4GB\") or percentage of system memory (e.g., \"20%\").\n## Setting it to 0 disables the limit (unbounded, default behavior).\n## When this limit is reached, queries will fail with ResourceExhausted error.\n## NOTE: This does NOT limit memory used by table scans.\nmemory_pool_size = \"25%\"\n\n[storage]\ndata_home = \"./greptimedb_data\"\ntype = \"File\"\ncache_capacity = \"5GiB\"\n\n[logging]\ndir = \"/logs\"\nlevel = \"info\"\nenable_otlp_tracing = false\nappend_stdout = true\nlog_format = \"json\"\nmax_log_files = 720\n\n[slow_query]\nenable = true\n\n</code></pre>"},{"location":"examples/vector_greptimedb/vector.yaml/","title":"Example Vector config","text":"<pre><code>data_dir: /var/vector\nschema:\n  log_namespace: false\n\napi:\n    enabled: true\n    address: 0.0.0.0:8686\n\nsources:\n\n  vector_logs:\n    type: \"internal_logs\"\n\n  otlp:\n    type: opentelemetry\n    grpc:\n      address: 0.0.0.0:4317\n    http:\n      address: 0.0.0.0:4318\n      headers: []\n      keepalive:\n        max_connection_age_jitter_factor: 0.1\n        max_connection_age_secs: 300\n    use_otlp_decoding: false\n\n  dockernuc_journald:\n    type: journald\n\n  dockernuc_docker:\n    type: docker_logs\n    exclude_containers:\n      - vector\n      - greptime\n\n  syslog_daemon:\n    type: syslog\n    address: 0.0.0.0:514\n    mode: udp\n\n  syslog_daemon_tcp:\n     type: syslog\n     address: 0.0.0.0:5514\n     mode: tcp\n\ntransforms:\n  vector_errors:\n    type: \"filter\"\n    inputs:\n      - \"vector_logs\"\n    condition: '.metadata.level != \"INFO\"'\n\n  remap_docker:\n    inputs:\n      - \"dockernuc_docker\"\n    type: \"remap\"\n    source: |\n      # Map container_name to appname\n      if exists(.container_name) {\n        .appname = .container_name\n        del(.container_name)\n      }\n      . = filter(object(.)) -&gt; |key, _value| {\n        !starts_with(key, \"label\")\n      }\n\n  remap_syslog:\n    inputs:\n       - \"syslog_daemon\"\n       - \"syslog_daemon_tcp\"\n    type:   \"remap\"\n    drop_on_error: false\n    drop_on_abort: true\n    source: |\n      # Ensure message is a string and strip ANSI color codes\n      .message = strip_ansi_escape_codes(string!(.message))\n\n      # Try to parse syslog format\n      parsed, err = parse_syslog(.message)\n\n      # remove Home Assistant from HAOS LogSpout since richer events received from Remote Logger\n      if .appname == \"homeassistant\" {\n        abort\n      }\n\n      if err == null {\n        . = merge(., parsed)\n        # Normalize msg -&gt; message\n        if exists(.msg) {\n          .message = .msg\n          del(.msg)\n        }\n      }\n\n      # Ensure consistent types matching schema\n      .procid = if exists(.procid) {\n        if is_string(.procid) {\n          to_int(.procid) ?? 0\n        } else if is_integer(.procid) {\n        .procid\n        } else {\n        0\n      }\n      } else {\n        0\n      }\n      # remove redundant timestamp in message\n      ts_match = parse_regex(.message, r'^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d+Z?\\s*(?P&lt;msg&gt;.*)') ?? {}\n      if exists(ts_match.msg) {\n        .message = string!(ts_match.msg)\n      }\n      .severity = if exists(.severity) { string!(.severity) } else { \"info\" }\n\n      # CRITICAL: Remove timestamp string - conflicts with greptime_timestamp\n      del(.timestamp)\n      del(.version)  # Also remove version if not needed\n\nsinks:\n  file_vector:\n    type: \"file\"\n    inputs:\n      - \"vector_errors\"\n    compression: \"none\"\n    path: \"/logs/internal_vector.log\"\n    encoding:\n      codec: \"json\"\n  greptime:\n    inputs:\n      - \"remap_syslog\"\n      - \"remap_docker\"\n      - \"otlp.logs\"\n      - \"dockernuc_journald\"\n    type: \"greptimedb_logs\"\n    endpoint: http://greptime:4000\n    dbname: public\n    compression: gzip\n    table: logs\n\n</code></pre>"},{"location":"developer/coverage/","title":"Coverage","text":""}]}